#!/bin/bas
#SBATCH -A TG-CCR140008
#SBATCH -J benchmark
#SBATCH -o benchmark.stdout
#SBATCH -n 1 -N 1
#SBATCH -p normal -mic
#SBATCH -t 05:00:00     

#----------------- Example Job Script ------------------
##SBATCH -n 32 -N 4 
##SBATCH -p normal-mic
##SBATCH -t 02:00:00
##SBATCH -A A-yourproject
# 
# export MIC_PPN=2
# export MIC_OMP_NUM_THREADS=60
# export OMP_NUM_THREADS=2
# ibrun.symm -m ./mpihello.mic -c ./mpihello.host 
# -------------------------------------------------------
#  
#  When submitted, the job script above will start 32 host MPI tasks spread
#  across 4 nodes plus 2 MPI tasks on 4 MIC cards resulting in 40 MPI
#  tasks total. Each MPI task on the host will use 2 threads/task while
#  each MPI task on the MIC will use 60 threads/task.
#   
#   Note that the tasks will be allocated in consecutive order on the nodes, e.g. 
#    
#    NODE1:  8 host tasks ( 0 - 7) :  2 MIC tasks ( 8 - 9)
#    NODE2:  8 host tasks (10 -17) :  2 MIC tasks (18 -19)
#    NODE3:  8 host tasks (20 -27) :  2 MIC tasks (28 -29)
#    NODE4:  8 host tasks (30 -37) :  2 MIC tasks (38 -39)
#     
#     You can change this task ordering to have CPU tasks first, followed by
#     MIC tasks by setting the environment variable MIC_MPI_HOST_INTERLEAVE=0

export MIC_PPN=1
export MIC_OMP_NUM_THREADS=4

source ~/.bashrc

# now for 100 ps
grompp -f grompp_minimize.mdp -maxwarn 10
ibrun.symm -m $HOME/programs/gromacs/5.0.4/bin/mdrun.mic
cp confout.gro conf.gro

# now for 100 ps
grompp -f grompp_equil_nvt.mdp -maxwarn 10
ibrun.symm -m $HOME/programs/gromacs/5.0.4/bin/mdrun.mic
#mpirun -np 12 mdrun_mpi

